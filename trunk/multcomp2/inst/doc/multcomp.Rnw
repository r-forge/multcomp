
\documentclass{article}

%%\VignetteIndexEntry{On Multiple Comparison Procedures for Linear Hypotheses}
%%\VignetteDepends{multcomp2}

\usepackage[utf-8]{inputenc}

\usepackage{amsfonts}
\usepackage{amstext}
\usepackage{amsmath}
\newcommand{\R}{\mathbb{R} }
\newcommand{\X}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cor}{\text{Cor}}
\newcommand{\abs}{\text{abs}}
\usepackage{natbib}

\newcommand{\Rpackage}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\Robject}[1]{\texttt{#1}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rcmd}[1]{\texttt{#1}}
\newcommand{\Roperator}[1]{\texttt{#1}}
\newcommand{\Rarg}[1]{\texttt{#1}}
\newcommand{\Rlevel}[1]{\texttt{#1}}


\begin{document}

<<setup, echo = FALSE, results = hide>>=
dig <- 4
options(width = 65, digits = dig)
library("multcomp2")
set.seed(290875)
@

\SweaveOpts{engine=R,eps=FALSE}

\title{Simultaneous Inference Procedures \\ 
       for General Linear Hypotheses}

\author{Torsten Hothorn}

\maketitle

\section{Introduction}

We consider a generalized linear model, where the response variable 
$\y \in \R^n$ is modelled by a linear combination
of a set of design variables. More specifically, the expectation 
of the response $\y$ is described by a link function $h$ applied to 
a linear combination of the \emph{design matrix} $\X \in \R^{n \times p}$:
\begin{eqnarray*}
\E(\y | \X) = h(\X \beta).
\end{eqnarray*}

Our primary target is simultaneous inference about 
\emph{general linear hypotheses} about the regression coefficients
\citep{Searle1971}, i.e, 
about linear functions of the parameter vector $\beta \in \R^p$:
\begin{eqnarray*}
H_0: \K \beta = \m
\end{eqnarray*}
where $\K$ is an full-rank $k \times p$ matrix with each row corresponding to
one specific hypothesis. Classically, $F$ tests are applied to test 
such hypotheses. However, we are not only interested in the 
\emph{global} hypothesis $H_0$ but in all partial
hypotheses defined by the rows $K_j, j = 1, \dots, k$, of $\K$ and
the elements of $\m = (m_1, \dots, m_k)$:
\begin{eqnarray*}
H_0^j: K_j \beta = m_j \text{ where } H_0 = \bigcap_{j = 1}^k H_0^j
\end{eqnarray*}
We only consider simultaneous inference procedures, both tests and confidence intervals,
which control the \emph{family-wise error rate} (FWE), that is the probability of 
incorrectly rejecting at least one hypothesis $H_0^j, j = 1, \dots, k$.

\subsection{Estimates of the Regression Coefficients}

Linear or generalized linear modelling functions provide us with 
an unbiased estimator $\hat{\beta}$ of $\beta$ including its 
$p \times p$ covariance matrix $\V(\hat{\beta})$. In case of the linear model 
(Gaussian family with identity link $h$), the estimate $\hat{\beta}$ follows
a multivariate normal distribution $\hat{\beta} \sim \mathcal{N}(\beta, \V(\hat{\beta}))$
and thus $\K \hat{\beta} \sim \mathcal{N}(\K \beta, \K \V(\hat{\beta}) \K^\top)$.
For generalized linear models, the multivariate normal distribution of $\hat{\beta}$
and $\K \hat{\beta}$ is valid only asymptotically.

\subsection{Simultaneous Tests and Confidence Intervals}

Under the conditions of the global hypothesis $H_0$ it holds that
\begin{eqnarray*}
\K \hat{\beta} - \m \sim \mathcal{N}(0, \K \V(\hat{\beta}) \K^\top),
\end{eqnarray*} 
either exactly (linear model) or asymptotically.
For each partial hypothesis $H_0^j$, a standardized test statistic 
of the form $z_j = K_j \hat{\beta} (K_j \V(\hat{\beta}) K_j^\top)^{-\frac{1}{2}}$
follows a $t$ distribution with $n - p$ degrees for freedom (linear model) 
or a asymptotical standard normal distribution (generalized linear model) and
tests or confidence intervals for the partial hypotheses can be constructed in
the classical way.
The vector of standarized statistics
\begin{eqnarray*}
\z = (z_1, \dots, z_k) = \Sigma (\K \hat{\beta} - \m) \text{ with }\Sigma = \text{diag}\left((\K \V(\hat{\beta}) \K^\top)^{-\frac{1}{2}}\right)
\end{eqnarray*}
follows a multivariate $t$ distribution with correlation matrix
\begin{eqnarray*}
\Cor(\K \hat{\beta}) = \Sigma \K \V(\hat{\beta}) \K^\top \Sigma^\top
\end{eqnarray*}
in case of a linear model:
\begin{eqnarray*}
\z \sim \mathcal{T}_{n - p}(0, \Cor(\K \hat{\beta}))
\end{eqnarray*}
For all other models, those statistics are jointly 
asymptotic multivariate normal:
\begin{eqnarray*}
\z \sim \mathcal{N}(0, \Cor(\K \hat{\beta})).
\end{eqnarray*}

Now, a multiple testing procedure is based on the maximum of the absolute values
of the test statistics $\z$. Adjusted $p$ values, controlling the family-wise error rate,
for each linear hypothesis $H_0^j$ are simply $p_j = P_{H_0}(\max(|\z|) \ge |z_j|)$.
Efficient algorithms for the evalutation of both multivariate distributions
are nowadays available \citep{Genz1992, GenzBretz1999, GenzBretz2002}.

\paragraph{Example: Simple Linear Model.}

Consider a simple univariate linear model regressing the distance to stop 
on speed for $\Sexpr{nrow(cars)}$ cars:
<<lm-cars, echo = TRUE>>=
lm.cars <- lm(dist ~ speed, data = cars)
summary(lm.cars)
@
The estimates of the regression coefficients $\beta$ and their covariance matrix
can be extracted from the fitted model via:
<<lm-coef-vcov, echo = TRUE>>=
betahat <- coef(lm.cars)
Vbetahat <- vcov(lm.cars)
@
At first, we are interested in the hypothesis $\beta_1 = 0 \text{ and } \beta_2 = 0$. 
This is equivalent
to the linear hypothesis $\K \beta = 0$ where $\K = \text{diag}(2)$, i.e.,
<<lm-K, echo = TRUE>>=
K <- diag(2)
Sigma <- diag(1 / sqrt(diag(K %*% Vbetahat %*% t(K)))) 
z <- Sigma %*% K %*% betahat
Cor <- Sigma %*% (K %*% Vbetahat %*% t(K)) %*% t(Sigma)                  
@
Note that $\z = \Sexpr{paste("(", paste(round(z, dig), collapse = ", "), ")")}$ 
is equal to the $t$ statistics. The multiplicity-adjusted
$p$ values can now be computed by means of the multivariate $t$ distribution
utilizing the \Rcmd{pmvt} function available in package \Rpackage{mvtnorm}:
<<lm-partial, echo = TRUE>>=
library("mvtnorm")
df.cars <- nrow(cars) - length(betahat)
sapply(abs(z), function(x) 1 - pmvt(-rep(x, 2), rep(x, 2), corr = Cor, df = df.cars))
@
Note that the $p$ value of the global test is the minimum $p$ value of the partial tests.

The computations above can be performed much more conveniently using the functionality
implemented in package \Rpackage{multcomp2}. The function \Rcmd{glht} just takes a fitted 
model and a matrix defining the linear functions, and thus hypotheses,
to be tested:
<<lm-K, echo = FALSE>>=
rownames(K) <- names(betahat)
@
<<lm-mcp, echo = TRUE>>=
library("multcomp2")
cars.ht <- glht(lm.cars, linfct = K)
summary(cars.ht)
@
Simultaneous confidence intervals corresponding to this multiple testing
procedure are available via
<<lm-confint, echo = TRUE>>=
confint(cars.ht)
@

\paragraph{Example: Confidence Bands for Regression Line.}

Suppose we want to plot the linear model fit to the \Robject{cars} data
including an assessment of the variability of the model fit. This can 
be based on simultaneous confidence intervals for the regression line $\X \hat{\beta}$,
i.e., we choose $\K = \X$:
<<lm-band, echo = TRUE>>=
K <- model.matrix(lm.cars)[!duplicated(cars$speed),]
ci.cars <- confint(glht(lm.cars, linfct = K), abseps = 0.1)
@
Figure \ref{lm-plot} depicts the regression fit together with the
confidence band for the regression line and the pointwise 
confidence intervals as computed by \Rcmd{predict(lm.cars)}.
\begin{figure}
\begin{center}
<<lm-plot, echo = FALSE, fig = TRUE, width = 8, height = 5>>=
plot(cars, xlab = "Speed (mph)", ylab = "Stopping distance (ft)",
            las = 1, ylim = c(-30, 130))
abline(lm.cars)
lines(K[,2], ci.cars$confint[,"lwr"], lty = 2)
lines(K[,2], ci.cars$confint[,"upr"], lty = 2)
ci.lm <- predict(lm.cars, interval = "confidence")
lines(cars$speed, ci.lm[,"lwr"], lty = 3)
lines(cars$speed, ci.lm[,"upr"], lty = 3)
legend("topleft", lty = c(1, 2, 3), legend = c("Regression line", 
                                               "Simultaneous confidence band", 
                                               "Pointwise confidence intervals"),
       bty = "n")
@
\caption{\Robject{cars} data: Regression line with confidence 
    bands (dashed) and intervals (dotted). \label{lm-plot}}
\end{center}
\end{figure}

\section{Multiple Comparison Procedures} 

Consider a one-way ANOVA model, i.e., the only covariate $\x_1$ is a factor at $l$ levels.
In this case, the design matrix $\X$ is the matrix of dummy codings of the factor
levels (in the absence of an intercept term):
<<aov-ex, echo = TRUE>>=
ex <- data.frame(y = rnorm(12), x = gl(3, 4, labels = LETTERS[1:3]))
aov.ex <- aov(y ~ x - 1, data = ex)
summary(aov.ex)
@
The elements of the regression coefficient 
$\beta = (\beta_1, \beta_2, \beta_3)$ correspond to the mean values of the
response variable in each group:
<<aov-coef, echo = TRUE>>=
coef(aov.ex)
@
Thus, the hypotheses $\beta_2 - \beta_1 = 0 \text{ and } \beta_3 - \beta_1 = 0$ can be written
in form of a linear function $\K \beta$ with
<<aov-Dunnett, echo = TRUE>>=
K <- rbind(c(-1, 1, 0),
           c(-1, 0, 1))
rownames(K) <- c("B - A", "C - A")
colnames(K) <- names(coef(aov.ex))
K
@
This "many-to-one" comparison procedure (Dunnett) is now simply
<<aov-mcp, echo = TRUE>>=
summary(glht(aov.ex, linfct = K))
@
Alternatively, a symbolic description of the general linear hypothesis
of interest can be supplied to \Rcmd{glht}:
<<aov-mcp2, echo = TRUE>>=
summary(glht(aov.ex, linfct = c("xB - xA = 0", "xC - xA = 0")))
@

In the presence of an intercept term, the full design matrix $\X$ can't be used to fit such
a model because the columns of $\X$ aren't independent of each other. Therefore,
a contrast matrix $\C$ is included into this model:
\begin{eqnarray*}
\X \beta = \X \C \beta^\star = \X^\star \beta^\star
\end{eqnarray*}
any we only obtain estimates for $\beta^\star$, but not for the
`original' parameter vector $\beta$:
<<aov-constrasts, echo = TRUE>>=
aov.ex2 <- aov(y ~ x, data = ex)
coef(aov.ex2)
@
The default contrasts are so-called treatment contrasts, nothing but differences in means
for one baseline group (compare the Dunnett contrasts and the estimated regression coefficients):
<<aov-mm, echo = TRUE>>=
contr.treatment(table(ex$x))
K %*% contr.treatment(table(ex$x)) %*% coef(aov.ex2)[-1]
@
so that $\K \C \hat{\beta}^\star = \K \hat{\beta}$.

However, the \Rcmd{glht} function takes care of contrasts and the matrix of
linear hypotheses $\K$ can be written in terms of $\beta$, not $\beta^\star$. Note
that the matrix of linear hypotheses only applies to those elements of $\hat{\beta}^\star$
attached to factor \Robject{x} but not to the intercept term $\hat{\beta}_1$:
<<aov-contrasts-glht, echo = TRUE>>=
summary(glht(aov.ex2, linfct = mcp(x = K)))
@
or, a little bit more convenient in this simple case:
<<aov-contrasts-glht2, echo = TRUE>>=
summary(glht(aov.ex2, linfct = mcp(x = c("B - A = 0", "C - A = 0"))))
@

More generally, inference on linear functions of parameters which can be
interpreted as `means' are known as \emph{multiple comparison procedures} 
(MCP). For some of the more prominent special cases, the corresponding
linear functions can be computed by convenience functions part
of \Rpackage{multcomp2}. For example, Tukey all-pair comparisons
for the factor \Robject{x} can be set up using
<<aov-Tukey>>=
glht(aov.ex2, linfct = mcp(x = "Tukey"))
@
The initial parameterization of the model is automatically taken into account:
<<aov-Tukey2>>=
glht(aov.ex, linfct = mcp(x = "Tukey"))
@


\bibliographystyle{plainnat}
\bibliography{multcomp}

\end{document}

