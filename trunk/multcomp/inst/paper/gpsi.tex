
\documentclass[12pt]{article}
\usepackage{a4wide}
\input{header}

\title{Generalized Simultaneous Inference \\ in Parametric Models}
\author{Torsten Hothorn, Frank Bretz, Peter Westfall}

\begin{document}

\maketitle

\section{Model and Estimation}

Let $\M(\Z_n, \vartheta, \eta)$ denote a (semi-)parametric statistical model
for observations $\Z_n = (\Z_1, \dots, \Z_n)$ with fixed
but unknown parameters of interest $\vartheta \in \R^p$ 
and other (random or nuisance) parameters $\eta$. We are provided 
with an estimate $\hat{\vartheta} \in \R^p$ of the parameter 
vector $\vartheta$ and are interested in a linear
function $f_{\K, \m}(\vartheta) = \K \vartheta - \m$
of the parameters defined by finite constants $\K \in \R^{k, p}$
and $\m \in \R^k$.

\paragraph{Model assumptions.}

We assume that the parameter estimates $\hat{\vartheta}$ converge in law
to a $p$-dimensional multivariate normal distribution:
\begin{eqnarray} \label{normality}
\hat{\vartheta} \cL \N_p(\vartheta, \Sigma)
\end{eqnarray}
with constant and finite covariance matrix $\Sigma \in \R^{p,p}$.
Since the covariance matrix is usually unknown, we assume 
that we are given a consistent estimate $\hat{\Sigma} \cP \Sigma$.

\paragraph{Limiting distribution of $f_{\K,\m}(\hat{\vartheta})$.}

The linear function $f_{\K, \m}$ is continuous and differentiable in
$\vartheta$ with Jacobian $\K$ and thus, by Cram{\'e}r's Theorem, 
the linear function $f$ of the parameter estimates
converges in law to a (possibly singular) $k$-dimensional normal distribution:
\begin{eqnarray} \label{dist_f}
f_{\K, \m}(\hat{\vartheta}) = \K \hat{\vartheta} - \m 
\cL \N_k(\K \vartheta - \m, \K \Sigma \K^\top).
\end{eqnarray}
Furthermore, it follows from the same theorem that we can 
consistently estimate the unknown covariance matrix $\K \Sigma \K^\top$ utilizing
the consistent estimate $\hat{\Sigma}$ of the covariance matrix:
\begin{eqnarray*}
\hat{\S} := \K \hat{\Sigma} \K^\top \cP \K \Sigma \K^\top =: \S.
\end{eqnarray*}

\paragraph{Test statistic and limiting distribution}

Let $\T := \hat{\D} (\K \hat{\vartheta} - \m) \in \R^k$ denote
the $k$-dimensional test statistic where $\hat{\D}$ is the diagonal
matrix of standard deviations
\begin{eqnarray*}
\hat{\D} := \text{diag}\left(\text{diag}(\hat{S})^{-1/2}\right) \cP 
\text{diag}\left(\text{diag}(S)^{-1/2}\right) =: \D \in \R^{k,k}
\end{eqnarray*}
In order to show that also $\T$ converges in law to a $k$-dimensional multivariate
normal distribution
\begin{eqnarray*}
\T \cL \N_k(\D(\K \vartheta - \m), \D \S \D^\top)
\end{eqnarray*}
we utilize the Cram{\'e}r-Wold device and show that for some $\u \in \R^k$ 
the linear function $\u^\top \T$ is univariate normal. Consider
\begin{eqnarray*}
\u^\top \T & = & \u^\top \hat{\D} (\K \hat{\vartheta} - \m) \\
& = & \left(u_1 \hat{\S}_{11}^{-1/2}, \dots, u_k \hat{\S}_{kk}^{-1/2}\right)^\top(\K \hat{\vartheta} - \m).
\end{eqnarray*}
Since $\hat{\S}$ is a consistent estimate of $\S$ it 
is clear that $u_j \hat{\S}_{jj}^{-1/2} \cP u_j \S_{jj}^{-1/2}$.
Together with (\ref{dist_f}) we can apply Slutsky's theorem to show that
\begin{eqnarray*}
u_j \hat{S}_{jj}^{-1/2} (\K \hat{\vartheta} - \m)_j & \cL &  
\N_1(u_j \S_{jj}^{-1/2} (\K \vartheta - \m)_j, u_j^2 \S_{jj}^{-1} \S_{jj}) \\
& = & \N_1(u_j \S_{jj}^{-1/2} (\K \vartheta - \m)_j, u_j^2)
\end{eqnarray*}
Because the above holds for every $\u \in \R^k$, the asymptotic multivariate
normaliy of $\T$ follows from the Cram{\'e}r-Wold device. In summary,
the vector of test statistics $\T$ converges in law to a multivariate
normal distribution with correlation matrix $\Cor := \D \S \D^\top \in \R^{k,k}$.

\section{Simultaneous Inference}

Under the hypothesis
\begin{eqnarray*}
H_0: \K \vartheta = \m
\end{eqnarray*}
$\T \cL \N_k(0, \Cor)$ holds true. At this point it is worth
considering two special cases. A stronger assumption than asymptotic normality
of $\hat{\vartheta}$ (\ref{normality}) is exact normality:
$\hat{\vartheta} \sim \N_p(\vartheta, \Sigma)$. Under the unrealistic
further assumption of the covariance matrix being known, it
is easy to show that $\T \sim \N_k(0, \Cor)$. 
When $\Sigma$ is unknown (the typical situtation of linear models
with normal iid errors and constant variance), the
exact distribution of $\T$ is a $k$-dimensional multivariate $\mathcal{T}$
distribution with $df$ degrees of freedom (df $ = n - p - 1$ for linear models).

Now, the distribution function of $\T$ can be calculated or approximated
by evaluating the $k$-dimensional exact or limiting distributions:
\begin{eqnarray*}
& & \Prob(\max(\T) \le t)  \approx \int\limits_{-\infty}^t \cdots \int\limits_{-\infty}^t 
\varphi_k(x_1, \dots, x_k; \Cor, \text{df}) \, dx_1 \cdots dx_k \quad \text{("less")}\\
& & \Prob(\max(|\T|) \le t)  \approx  \int\limits_{-t}^t \cdots \int\limits_{t}^t 
\varphi_k(x_1, \dots, x_k; \Cor, \text{df}) \, dx_1 \cdots dx_k \quad \text{("two sided")}\\
& & \Prob(\min(\T) \ge t) \approx  \int\limits_{t}^\infty \cdots \int\limits_{t}^\infty 
\varphi_k(x_1, \dots, x_k; \Cor, \text{df}) \, dx_1 \cdots dx_k \quad \text{("greater")}\\
\end{eqnarray*}
where $\varphi_k$ is either the density function of the limiting multivariate
normal (df $ = 1$) or exact multivariate $\mathcal{T}$-distribution (df $ > 1$).
Since $\Cor$ is usually unknown, we plug-in an consistent estimate
$\hat{\Cor} := \hat{\D} \hat{\S} \hat{\D}^\top$.

\paragraph{Simultaneous Tests.}

An adjusted $p$-value for the $j$th partial hypothesis 
\begin{eqnarray*}
H_0^j: (\K \vartheta)_j = \m_j
\end{eqnarray*}
is given by $1 - \Prob(\max(|\T|) \le |t_j|)$ when the test statistic 
$\t  = (t_1, \dots, t_k)$
has been observed. The global $p$-value for $H_0$ is $1 - \Prob(\max(|\T|) \le \max|t_j|)$.
Alternatively, a quadratic form based on the Moore-Penrose inverse $\hat{\S}^+$ of 
$\hat{\S}$ or the usual $F$-test (assuming exact normality of $\hat{\vartheta}$) 
can be utilized to construct a global test:
\begin{eqnarray*}
X^2 & = & (\K \hat{\vartheta} - \m)^\top \hat{\S}^+ (\K \hat{\vartheta} - \m) \cL \chi^2_{\Rg(\hat{\S})} \\
F & = &  \frac{X^2}{\Rg(\hat{\S})} \sim \F_{\text{df}, \Rg(\hat{\S})}
\end{eqnarray*}

\paragraph{Simultaneous confidence sets.}

A simultaneous $(1 - 2\alpha) \times 100\%$ 
confidence set for $\K \vartheta$ is given by 
$\K \hat{\vartheta} \pm q_\alpha \text{diag}(\hat{\S})^{-1/2}$
where $q_\alpha$ is the $1 - \alpha$ 
quantile of the distribution of $\T$ 
such that $\Prob(\max{\T} \le q_\alpha) \ge 1 - \alpha$.

\section{Applications}

\subsection{One-way ANOVA}

One-way ANOVA for three groups:
$Y_{ij} = \mu + \alpha_{j} + \varepsilon_{ij}, j = 1, 2, 3$,
with $\vartheta = (\mu, \alpha_2 - \alpha_1, \alpha_3 - \alpha_1)$.

Dunnett: 
\begin{eqnarray*}
\K = (0, \diag(2))
\end{eqnarray*}
and $\K \vartheta = (\alpha_2 - \alpha_1, \alpha_3 - \alpha_1)$.

\subsection{GLM}

\subsection{(N)LME}

\subsection{Cox, Weibull, ...}

\subsection{Robust}

Use sandwich estimator and asymptotic normal.

\end{document}

