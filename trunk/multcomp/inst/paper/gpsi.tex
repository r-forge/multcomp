
\documentclass[12pt]{article}
\usepackage{a4wide}
\usepackage[lists,heads,nomarkers]{endfloat}

\input{header}

\hypersetup{%
  pdftitle = {Generalized Simultaneous Inference in Parametric Models},
  pdfsubject = {Manuscript},
  pdfauthor = {Torsten Hothorn and Frank Bretz and Peter Westfall},
%% change colorlinks to false for pretty printing
  colorlinks = {true},
  linkcolor = {blue}, 
  citecolor = {blue}, 
  urlcolor = {red},   
  hyperindex = {true},
  linktocpage = {true},
}
 


\title{Generalized Simultaneous Inference \\ in Parametric Models}
\author{\textbf{Torsten Hothorn} \\
Institut f{\"u}r Statistik \\
Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen \\
Ludwigstra{\ss}e 33, D--80539 M{\"u}nchen, Germany\\
\and \textbf{Frank Bretz} \\
Biostatistics \& Statistical Reporting, WSJ-27.1.005\\
Novartis Pharma AG \\
CH-4002 Basel, Switzerland \\
\and \textbf{Peter Westfall} \\
Texas Tech University \\
Lubbock, TX 79409, U.S.A}

\begin{document}

\maketitle

\begin{abstract}
Abstract
\end{abstract}

\textbf{Key words}: multiple tests, multiple comparisons, confidence intervals,
adjusted $p$-values, multivariate normal distribution, robust statistics.

\newpage

\section{Introduction}


The multiplicity problem arises when several inferences are considered
simultaneously as a group.  If each inference has a $5\%$ error rate,   
then the error rate over the entire group can be much higher than $5\%$.
In this paper we aim at a unified treatment of simultaneous
inference procedures in parametric and semi-parametric
models where parameter estimates are generally correlated.
Each partial hypothesis is described by a linear
combination of the model parameters and we allow
for $k$ of such hypotheses to be tested simultaneously,
either using $p$-values or confidence intervals
for the linear functions. Maybe the most prominent
and practially interesting applications are multiple
comparison procedures where the model parameters
refer to groups such as treatments, for example.
Popular approaches include Dunnett's many-to-one comparisons,
Tukey's honest significant differences, sequential pairwise contrasts, 
comparisons with the average, changepoint analysis, Williams', 
Marcus', or McDermott's contrasts. These procedures are
all defined for the classical balanced one- or two-way ANOVA model
with iid normal errors and constant variance.

The general framework for simultaneous inference improves
upon contemporary methodology with respect to the following
issues: (i) model assumptions like normality and homoscedasticity
are relaxed which opens up the way to simultaneous inference
in arbitrary models, such as generalized linear models, mixed models,
survival models etc. (ii) arbitrary linear functions of the
parameters are allowed, not just contrasts. (iii)
a unified implementation allowing for a fast transition of the
theoretical results to the desks of data analysts interested
in honest inference for multiple hypotheses is provided.

The paper is organized as follows. In Sections~\ref{model} and \ref{siminf} we
describe a unified framework for generalized simultaneous inference
procedures. An overview on the most important applications
of the methodology is given in Section~\ref{applications} followed
by a short discussion of a software implementation in Section~\ref{implementation}.
Most interesting from a practical point of view is 
Section~\ref{illustrations} where we analyze four rather challenging
problems with the tools developed in this paper.

\section{Model and Estimation} \label{model}

Let $\M(\Z_n, \vartheta, \eta)$ denote a (semi-)parametric statistical model.
The vector of $n$ observations is denoted by $\Z_n = (\Z_1, \dots, \Z_n)$. 
The model is equiped with fixed
but unknown parameters of interest $\vartheta \in \R^p$ 
and other (random or nuisance) parameters $\eta$. We are provided 
with an estimate $\hat{\vartheta} \in \R^p$ of the parameter 
vector $\vartheta$ and are interested in a linear
function $f_{\K, \m}(\vartheta) = \K \vartheta - \m$
of the parameters defined by finite constants $\K \in \R^{k, p}$
and $\m \in \R^k$.

\paragraph{Model assumptions.}

We assume that the parameter estimates $\hat{\vartheta}$ converge in law
to a $p$-dimensional multivariate normal distribution:
\begin{eqnarray} \label{normality}
\hat{\vartheta} \cL \N_p\left(\vartheta, \Sigma\right)
\end{eqnarray}
with constant and finite covariance matrix $\Sigma \in \R^{p,p}$.
Since the covariance matrix is usually unknown, we assume 
that we are given a consistent estimate $\hat{\Sigma} \cP \Sigma$.

\paragraph{Limiting distribution of $f_{\K,\m}(\hat{\vartheta})$.}

The linear function $f_{\K, \m}$ is continuous and differentiable in
$\vartheta$ with Jacobian $\K$ and thus, by Cram{\'e}r's Theorem, 
the linear function $f$ of the parameter estimates
converges in law to a (possibly singular) $k$-dimensional normal distribution:
\begin{eqnarray} \label{dist_f}
f_{\K, \m}(\hat{\vartheta}) = \K \hat{\vartheta} - \m 
\cL \N_k\left(\K \vartheta - \m, \K \Sigma \K^\top\right).
\end{eqnarray}
Furthermore, it follows from the same theorem that we can 
consistently estimate the unknown covariance matrix $\K \Sigma \K^\top$ utilizing
the consistent estimate $\hat{\Sigma}$ of the covariance matrix $\Sigma$:
\begin{eqnarray*}
\hat{\S} := \K \hat{\Sigma} \K^\top \cP \K \Sigma \K^\top =: \S.
\end{eqnarray*}

\paragraph{Test statistic and limiting distribution.}

Let $\T := \hat{\D} (\K \hat{\vartheta} - \m) \in \R^k$ denote
the $k$-dimensional standardized test statistic where $\hat{\D}$ is the diagonal
matrix of standard deviations
\begin{eqnarray*}
\hat{\D} := \text{diag}\left(\text{diag}(\hat{\S})^{-1/2}\right) \cP 
\text{diag}\left(\text{diag}(\S)^{-1/2}\right) =: \D \in \R^{k,k}
\end{eqnarray*}
In order to show that also $\T$ converges in law to a $k$-dimensional multivariate
normal distribution
\begin{eqnarray*}
\T \cL \N_k\left(\D(\K \vartheta - \m), \Cor := \D \S \D^\top\right)
\end{eqnarray*}
we utilize the Cram{\'e}r-Wold device and show that for some $\uu \in \R^k$ 
the linear function $\uu^\top \T$ is univariate normal. Consider
\begin{eqnarray*}
\uu^\top \T & = & \uu^\top \hat{\D} (\K \hat{\vartheta} - \m) \\
& = & \left(u_1 \hat{\S}_{11}^{-1/2}, \dots, u_k \hat{\S}_{kk}^{-1/2}\right)^\top(\K \hat{\vartheta} - \m).
\end{eqnarray*}
Since $\hat{\S}$ is a consistent estimate of $\S$ it 
is follows that $u_j \hat{\S}_{jj}^{-1/2} \cP u_j \S_{jj}^{-1/2}$.
Together with (\ref{dist_f}) we can apply Slutsky's theorem to show that
\begin{eqnarray*}
u_j \hat{\S}_{jj}^{-1/2} (\K \hat{\vartheta} - \m)_j & \cL &  
\N_1\left(u_j \S_{jj}^{-1/2} (\K \vartheta - \m)_j, u_j^2 \S_{jj}^{-1} \S_{jj}\right) \\
& = & \N_1\left(u_j \S_{jj}^{-1/2} (\K \vartheta - \m)_j, u_j^2\right)
\end{eqnarray*}
Because the above holds for every $\uu \in \R^k$, the asymptotic multivariate
normality of $\T$ follows from the Cram{\'e}r-Wold device. Note that
all elements of $\T$ have mean zero and variance $1$. In summary,
the vector of test statistics $\T$ converges in law to a multivariate
normal distribution with correlation matrix 
$\Cor := \D \S \D^\top \in \R^{k,k}$.

\section{Simultaneous Inference} \label{siminf}

Under the general linear hypothesis \citep{Searle1971}
\begin{eqnarray*}
H_0: \K \vartheta = \m
\end{eqnarray*}
it has been shown in Section~\ref{model} that 
$\T \cL \N_k(0, \Cor)$ holds true. At this point it is worth
considering two special cases. A stronger assumption than asymptotic normality
of $\hat{\vartheta}$ (\ref{normality}) is exact normality, i.e., 
$\hat{\vartheta} \sim \N_p(\vartheta, \Sigma)$. Under the unrealistic
further assumption of the covariance matrix $\Sigma$ being known, it
follows by standard arguments that $\T \sim \N_k(0, \Cor)$. 
When $\Sigma$ is unknown (the typical situation of linear models
with normal iid errors and constant variance), the
exact distribution of $\T$ is a $k$-dimensional multivariate $\mathcal{T}$
distribution with $df$ degrees of freedom (df $ = n - p - 1$ for linear models).
\marginpar{Ref}

Now, the distribution function of $\T$ can be calculated or approximated
by evaluating the $k$-dimensional exact or limiting distributions for some $t \in R$:
\begin{eqnarray*}
& & \Prob(\max(|\T|) \le t)  \cong  \int\limits_{-t}^t \cdots \int\limits_{t}^t 
\varphi_k(x_1, \dots, x_k; \Cor, \text{df}) \, dx_1 \cdots dx_k \quad \text{ ("two sided")}\\
& & \Prob(\max(\T) \le t)  \cong\int\limits_{-\infty}^t \cdots \int\limits_{-\infty}^t 
\varphi_k(x_1, \dots, x_k; \Cor, \text{df}) \, dx_1 \cdots dx_k \quad \text{("less")}\\
& & \Prob(\min(\T) \ge t) \cong  \int\limits_{t}^\infty \cdots \int\limits_{t}^\infty \varphi_k(x_1, \dots, x_k; \Cor, \text{df}) \, dx_1 \cdots dx_k \quad \text{("greater")}\\
\end{eqnarray*}
where $\varphi_k$ is either the density function of the limiting multivariate
normal (df $ = 1$ and `$\approx$') or exact multivariate $\mathcal{T}$-distribution (df $ > 1$
and `='). 
Since $\Cor$ is usually unknown, we plug-in a consistent estimate
$\hat{\Cor} := \hat{\D} \hat{\S} \hat{\D}^\top$. Efficient methods
for approximating the above multivariate normal and $\mathcal{T}$ 
probabilities are described in \cite{Genz1992,GenzBretz1999,BretzGenzHothorn2001}
and \cite{GenzBretz2002}. Alan Genz' implementations of these algorithms
are available in \RR{} from package \Rpackage{mvtnorm} 
\citep{pkg:mvtnorm,Rnews:Hothorn+Bretz+Genz:2001}.

\paragraph{Simultaneous tests.}

An adjusted $p$-value for the $j$th partial hypothesis 
\begin{eqnarray*}
H_0^j: (\K \vartheta)_j = \m_j
\end{eqnarray*}
is given by $1 - \Prob(\max(|\T|) \le |t_j|)$ when the test statistic 
$\tt  = (t_1, \dots, t_k)$
has been observed. More precisely, the adjusted $p$-values for different
directions of the alternative $H_1$ are given as follows:
\begin{eqnarray*}
& & H_0: \K \vartheta = \m \text{ vs. } H_1: \K \vartheta \not = \m \quad \Rightarrow \quad p_j  =  1 - \Prob(\max(|\T|) \le t) \quad \text{("two.sided")} \\
& & H_0: \K \vartheta \ge \m \text{ vs. } H_1: \K \vartheta < \m  \quad \Rightarrow \quad p_j  =  1 - \Prob(\max(\T) \le t) \quad \text{("less")} \\
& & H_0: \K \vartheta \le \m \text{ vs. } H_1: \K \vartheta > \m \quad \Rightarrow \quad p_j  =  1 - \Prob(\min(\T) \ge t) \quad \text{("greater")} \\
\end{eqnarray*}
The global $p$-value for a two-sided test problem is $1 - \Prob(\max(|\T|) \le \max|t_j|)$.


Alternatively, a quadratic form based on the Moore-Penrose inverse $\hat{\S}^+$ of 
$\hat{\S}$ or the usual $F$-test (assuming exact normality of $\hat{\vartheta}$) 
can be utilized to construct a global test:
\begin{eqnarray*}
X^2 & = & (\K \hat{\vartheta} - \m)^\top \hat{\S}^+ (\K \hat{\vartheta} - \m) \cL \chi^2_{\Rg(\hat{\S})} 
\quad \text{for } \hat{\vartheta} \cL \N_p(\vartheta, \Sigma) \\
F & = &  \frac{X^2}{\Rg(\hat{\S})} \sim \F_{\text{df}, \Rg(\hat{\S})} \quad \text{for } \hat{\vartheta} \sim \N_p(\vartheta, \Sigma)
\end{eqnarray*}

\paragraph{Simultaneous confidence sets.}

A simultaneous $(1 - 2\alpha) \times 100\%$ 
confidence set for $\K \vartheta$ is given by 
$\K \hat{\vartheta} \pm q_\alpha \text{diag}(\hat{\S})^{-1/2}$
where $q_\alpha$ is the $1 - \alpha$ 
quantile of the distribution of $\T$ 
such that $\Prob(\max{\T} \le q_\alpha) \ge 1 - \alpha$.

\section{Applications} \label{applications}

The framework described above is rather general and applicable 
to a wide range of models since most estimates are at least asymptotically
normal and a consistent estimate of their covariance matrix
is usually available and readily implemented. Most prominent
representatives are models fitted by maximum likelihood, M-estimation,
restricted maximum likelihood etc. \marginpar{more!}
It should be noted that we didn't make assumptions on the form
of $\K$ whatsoever and thus a rather straightforward construction 
principle for new simultaneous inference procedures is available. In
what follows, we assume $\m = 0$ only for the sake of simplicity.
The next paragraphs highlight a subjective selection of 
some special cases of practical importance.

\paragraph{One-way ANOVA.}

Consider a one-way ANOVA model for a factor measured at $q$ levels
with a continuous response and normal errors $\varepsilon_{ij} \sim \N_1(0, \sigma^2)$, 
i.e., the model
\begin{eqnarray*}
Y_{ij} = \mu + \alpha_{j} + \varepsilon_{ij}, j = 1,\dots,q \, i = 1, \dots, n_j.
\end{eqnarray*}
The model is overparameterized and thus only the parameters
$\vartheta = (\mu, \alpha_2 - \alpha_1, \alpha_3 - \alpha_1, \dots, \alpha_q - \alpha_1)$ 
are  estimated by classical least-squares. (Note that other contrasts can be used
but theses so-called `treatment contrasts' are the most prominent ones and,
for example, the default in \RR.) 
Most of the classical approaches
to multiple comparisons can be embedded into this simple framework and 
the two most prominent ones are Dunnett's many-to-one procedure and
Tukey's honest significant differences. For Dunnett's procedures,
the differences of all coefficients with $\alpha_1$ are under test, i.e.,
a linear function
\begin{eqnarray*}
\K_\text{Dunnett} = (0, \diag(q))
\end{eqnarray*}
with so called `contrasts' 
$\K_\text{Dunnett} \vartheta = (\alpha_2 - \alpha_1, \alpha_3 - \alpha_1, \dots, \alpha_q - \alpha_1)$.
For Tukey's honest significant differences the interest is in inference on
all possible differences of the model parameters $\alpha_1, \dots, \alpha_q$, i.e., 
the matrix (here for $q = 3$)
\begin{eqnarray*}
\K_\text{Tukey} = \left( 
\begin{array}{rrr} 0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 1 & -1 
\end{array} \right)
\end{eqnarray*}
with $\K_\text{Tukey} \vartheta = (\alpha_2 - \alpha_1, \alpha_3 - \alpha_1, \alpha_2 - \alpha_3)$.
Many other contrasts have been studies in the past, such as Williams, McDermott, Marcus,
Sequential, Changepoints etc. pp. \marginpar{Ref!} and all can be formulated in terms
of a linear function $\K \vartheta$.

Under the standard assumptions of ANOVA models, i.e., iid normal errors with constant
variance, the vector of test statistics $\T$ follows an exact $\mathcal{T}$-distribution and
thus, simultaneous tests and confidence intervals are `exact' as well.

\paragraph{GLM, (N)LME and survival models.}

In generalized linear models, the exact distribution of the parameter estimates
is usually unknown and thus the asymptotic normal distribution is the basis for all
inference procedures. When we are interested in inference about model
parameters corresponding to levels of a certain factor, the same multiple 
comparison procedures as sketched above are available. 

Linear and non-linear mixed effects models fitted by REML provide
the data analyst with asymptotically normal errors and a consistent
covariance matrix as well so that all assumptions of our framework
are met and one can set up simultaneous inference procedures
for these models as well.

The same is true for the Cox model or other parametric survival models
such as the Weibull model.

\paragraph{Robust simultaneous inference.}

An interesting possibility is to move to robust variants of the above
mentioned models. One obvious possibility is the application
of sandwich estimators $\hat{\Sigma}$ of the covariance matrix $\Sigma$, 
for example when the variance homogeneity in linear models is violated.
An even more robust approach is to use robust linear models, for example
fitted using MM.

The reader is referred to Section~\ref{illustrations} for some practical
instances of these ideas.

\section{Implementation} \label{implementation}

The general framework for simultaneous inference in 
(semi-)parametric models described in Section~\ref{model}
opens up the way to a unified implementation of simultaneous inference
procedures for arbitrary models meeting the model assumptions 
(\ref{normality}). We provide an \RR{} \citep{R2007} package called 
\Rpackage{multcomp} \citep{pkg:multcomp} implementing
simultaneous tests and confidence intervals for a wide range of models.

In \RR, estimated model coefficients $\hat{\vartheta}$ and their covariance 
matrix $\hat{\Sigma}$ are accessible by the model's \Rcmd{coef()} 
and \Rcmd{vcov()} methods. Thus, it is sufficient to assume both
methods being available for the model of interest. The \underline{g}eneral
\underline{l}inear \underline{h}ypo\underline{t}hesis for a model 
\Robject{model} and a representation of the linear function $\K$
called \Robject{linfct} is set up using \Rcmd{glht()}:
\begin{Sinput}
glht(model, linfct, alternative = c("two.sided", "less", "greater"), 
     rhs = 0, ...)
\end{Sinput}
The matrix $\K$ can be described in three ways
\begin{itemize}
\item by a matrix with \Rcmd{length(coef(model))} columns,
\item by an expression or character vector giving a symbolic description 
      of the linear functions of interest or
\item by an object of class \Rclass{mcp} 
      (for \underline{m}ultiple \underline{c}omparison \underline{p}rocedure).
\end{itemize}
The latter one is convenient when contrasts of levels of a factor are
to be compared and the model contrasts used to define the design matrix
of the model have to be taking into account. The \Rcmd{mcp()} function
takes the name of the factor under test as argument and a character 
defining the type of comparisons as its value. For example,
\Rcmd{mcp(type = "Tukey")} sets up a matrix $\K$ for Tukey's honest
significant differences in all levels of a factor \Robject{type}
which has to be part of right hand side of the model formula
of \Robject{model}. In this particular case, we need to assume
that \Rcmd{model.frame()} and \Rcmd{model.matrix()} methods for 
\Robject{model} are available as well. The two remaining arguments
\Rarg{alternative} and \Rarg{rhs} define the direction
of the alternative (see Section~\ref{model}) and $\m$, respectively.

Objects of class \Rclass{glht} returned by \Rcmd{glht()} are equiped with
\Rcmd{coef()} and \Rcmd{vcov()} methods for computing
$\K \hat{\vartheta}$ and $\hat{\S}$. Furthermore, a \Rcmd{summary()} method
is available for computing various flavors of tests (maximum-type, $X^2$ and $F$
tests) and adjusted $p$-values including those taking 
logical constraints into account \citep{Shaffer1986, Westfall1997}.
In addition, the \Rcmd{confint()} method applied to objects of class 
\Robject{glht} returns simultaneous confidence intervals and allows
for a graphical representation of the results.

The technical details of the implementation are not very interesting
at this point and we hope that the basic principles become much
clearer by means of the illustrations presented in the next section.
The interested reader is refered to the online documentation shipped
with package \Rpackage{multcomp}.

\input{illustrations}

\input{trees}

\section{Discussion}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
