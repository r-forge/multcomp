
\documentclass[12pt]{article}
\usepackage{a4wide}
\usepackage[lists,heads]{endfloat}

\input{header}

\hypersetup{%
  pdftitle = {Simultaneous Inference in General Parametric Models},
  pdfsubject = {Manuscript},
  pdfauthor = {Torsten Hothorn and Frank Bretz and Peter Westfall},
%% change colorlinks to false for pretty printing
  colorlinks = {true},
  linkcolor = {blue},
  citecolor = {blue},
  urlcolor = {red},
  hyperindex = {true},
  linktocpage = {true},
}

\title{Simultaneous Inference \\ in General Parametric Models}
\author{\textbf{Torsten Hothorn} \\
Institut f{\"u}r Statistik \\
Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen \\
Ludwigstra{\ss}e 33, D--80539 M{\"u}nchen, Germany\\
\and \textbf{Frank Bretz} \\
Statistical Methodology, Clinical Information Sciences\\
Novartis Pharma AG \\
CH-4002 Basel, Switzerland \\
\and \textbf{Peter Westfall} \\
Texas Tech University \\
Lubbock, TX 79409, U.S.A}

%% ToDo

%% Introduction: more elaborate formulations
%% add some references to recently published technology

%% Section 2  / 3: Model & Inference
%% check
%% reference for multivariate T distribution and
%% its applications

%% Section 4: Applications
%% references for M, S, REML etc. estimation

%% Section 5: Implementation
%% check

%% Section 6: Illustrations:
%% check

%% Section 7: Discussion
%% some comments on why we saved the world today.
%% maybe some more references

\begin{document}


\maketitle
\thispagestyle{empty}
\setcounter{page}{0}

\begin{abstract}
Simultaneous inference is a common problem in many
areas of application. If multiple null
hypotheses are tested simultaneously, the probability of rejecting
erroneously at least one of them increases beyond the pre-specified
significance level. Simultaneous inference procedures have to be used
which adjust for multiplicity and thus control the overall type I error
rate. In this paper we describe simultaneous
inference procedures in general parametric models, where the
experimental questions are specified through a linear
combination of the model parameters. The framework described here
is quite general and extends the canonical theory of multiple comparison procedures
in ANOVA models to linear regression problems, generalized linear models, linear mixed
effects models, the Cox model, robust linear models, etc.
Several examples using a variety of different statistical models
illustrate the breadth of the results. For the analyses we use the
\RR{} add-on package \Rpackage{multcomp}, which provides a convenient interface to
the general approach adopted here.

\end{abstract}

\textbf{Key words}: multiple tests, multiple comparisons, simultaneous
confidence intervals, \\
adjusted $p$-values, multivariate normal distribution, robust statistics.

\newpage

\section{Introduction}

Multiplicity is an intrinsic problem of any simultaneous inference.
If each of $k$, say, null hypotheses is tested at nominal level $\alpha$,
the overall type I error rate can be substantially larger than $\alpha$.
That is, the probability of at least one erroneous rejection is larger than $\alpha$ for
$k \geq 2$. Common multiple comparison procedures adjust for multiplicity and thus
ensure that the overall type I error remains below the pre-specified significance level $\alpha$.
Examples of such multiple comparison procedures include Dunnett's many-to-one comparisons,
Tukey's all-pairwise differences, sequential pairwise contrasts, comparisons with the
average, changepoint analyses, dose-response contrasts, etc. These procedures are
all well established for classical regression and ANOVA models allowing for covariates
and/or factorial treatment structures with i.i.d.~normal errors and constant
variance, see \cite{Bretzetal2008} and the references therein. For a general reading
on multiple comparison procedures we refer to
\cite{HochbergTamhane1987} and \cite{Hsu1996}.

In this paper we aim at a unified description of simultaneous
inference procedures in parametric models with generally correlated parameter estimates.
Each individual null hypothesis is specified through a linear
combination of the model parameters and we allow
for $k$ of such null hypotheses to be tested simultaneously.
The general framework described here extends the current canonical theory
with respect to the following aspects: (i) model assumptions, such as normality
and homoscedasticity
are relaxed, thus allowing for simultaneous inference
in generalized linear models, mixed-effects models,
survival models, etc.; (ii) arbitrary linear functions of the
parameters are allowed, not just contrasts of means in AN(C)OVA models; (iii)
computing the reference distribution is feasible for arbitrary designs,
especially unbalanced designs; and (iv)
a unified implementation is provided which allows for a fast transition of the
theoretical results to the desks of data analysts interested
in simultaneous inferences for multiple hypotheses.

Accordingly, the paper is organized as follows.
Section~\ref{model} defines the general model and obtains
the asymptotic or exact distribution of linear functions
of model parameters under rather weak conditions.
In Section~\ref{siminf} we
describe the framework for simultaneous inference
procedures in general parametric models. An overview about important applications
of the methodology is given in Section~\ref{applications} followed
by a short discussion of the software implementation in Section~\ref{implementation}.
Most interesting from a practical point of view is
Section~\ref{illustrations} where we analyze four rather challenging
problems with the tools developed in this paper.


\section{Model and Estimation} \label{model}

In this section we introduce the underlying model assumptions
and derive some asymptotic results necessary in the subsequent sections.
The results from this section form the basis for the
simultaneous inference procedures described in Section~\ref{siminf}.

Let $\M(\Z_n, \vartheta, \eta)$ denote a (semi-)parametric statistical
model.
The set of $n$ observations is described by $\Z_n = (\Z_1, \dots, \Z_n)$.
The model contains fixed
but unknown parameters of interest $\vartheta \in \R^p$
and other (random or nuisance) parameters $\eta$.
We are interested in the linear
functions $\K \vartheta$
of the parameter vector $\vartheta$ as specified through the constant matrix
$\K \in \R^{k, p}$.

Assume that we are given
an estimate $\hat{\vartheta} \in \R^p$ of the parameter
vector $\vartheta$. In what follows we describe the underlying model assumptions,
the limiting distribution of $\K\hat{\vartheta}$ as well as the 
corresponding test statistics and their limiting joint distribution.



\paragraph{Model assumptions.}

We assume that the parameter estimates $\hat{\vartheta}$ converge in law
to a $p$-dimensional normal distribution as the sample
size $n$ tends to infinity, that is,
\begin{eqnarray} \label{normality}
\sqrt{n}\left(\hat{\vartheta} - \vartheta\right) \cL \N_p\left(0, \Sigma\right)
\end{eqnarray}
with constant and finite covariance matrix $\Sigma \in \R^{p,p}$.
Since the covariance matrix is usually unknown, we assume
that we are given a consistent estimate $\hat{\Sigma} \cP \Sigma$.

Since in the following we only assume that the parameters estimates are
asymptotically normally distributed with
a consistent estimate of the associated covariance matrix
being available, our
framework covers a large class of statistical models, including linear
regression and ANOVA models, generalized linear models, linear mixed
effects models, the Cox model, robust linear models, etc.
Standard software packages can be used to fit such models
and obtain the estimates $\hat{\vartheta}$ and $\hat{\Sigma}$ which
are essentially the only two quantities that are needed for what follows.
It should be noted that the model parameters $\vartheta$ are not necessarily
means or differences of means in AN(C)OVA models.
Also, we do not restrict our attention to contrasts of such means, but allow for
any set of constants leading to the linear functions $\K \vartheta$ of interest.
Specific examples for $\K$ and $\vartheta$ will be given later in Section~\ref{illustrations}.



\paragraph{Limiting distribution of $\K\hat{\vartheta}$.}

The linear function $\K \vartheta$ is continuous and differentiable in
$\vartheta$ and thus, by Theorem 5.1.5 in \cite{Lehmann1999} and
standard arguments for linear transformations of multivariate normal
variables,
the linear function $\K \vartheta$ of the parameter estimates
converges in law to a (possibly singular) $k$-dimensional normal
distribution:
\begin{eqnarray} \label{dist_f}
\sqrt{n} \left(\K \hat{\vartheta} - \K \vartheta\right)
\cL \N_k\left(0, \K \Sigma \K^\top\right).
\end{eqnarray}
Furthermore, because the quadratic form $\K \hat{\Sigma} \K^\top$
is continuous and, by assumption, $\hat{\Sigma} \cP \Sigma$,
it follows from Theorem 5.1.1 in \cite{Lehmann1999} that we can
consistently estimate the unknown covariance matrix $\K \Sigma \K^\top$ utilizing
a consistent estimate $\hat{\Sigma}$ of the covariance matrix $\Sigma$:
\begin{eqnarray*}
\hat{\S} := \K \hat{\Sigma} \K^\top \cP \K \Sigma \K^\top =: \S.
\end{eqnarray*}
Anticipating that asymptotically non-degenerate linear functions of the
data are of interest, we assume that the diagonal elements of $S$ are
strictly positive.

\paragraph{A multivariate statistic and its limiting distribution.}

Let $\T := \sqrt{n} \hat{\D} (\K \hat{\vartheta} - \K \vartheta) \in \R^k$ denote
the $k$-dimensional standardized statistic, where $\hat{\D}$ is the diagonal
matrix of standard deviations, that is,
\begin{eqnarray*}
\hat{\D} := \text{diag}\left(\text{diag}(\hat{\S})^{-1/2}\right) \cP
\text{diag}\left(\text{diag}(\S)^{-1/2}\right) =: \D \in \R^{k,k}.
\end{eqnarray*}
Note that $\T$ converges in law to a $k$-dimensional multivariate
normal distribution, that is,
\begin{eqnarray*}
\T \cL \N_k\left(0, \Cor\right),
\end{eqnarray*}
where $ \Cor := \D \S \D^\top$. To see this we utilize the
Cram{\'e}r-Wold device \citep[e.g., Theorem 5.1.8
in][]{Lehmann1999} and show that for any $\uu \in \R^k$
the linear function $\uu^\top \T$ is univariate normal. Consider
\begin{eqnarray*}
\uu^\top \T & = & \uu^\top \sqrt{n} \hat{\D} (\K \hat{\vartheta} - \K \vartheta) \\
& = & \sqrt{n} \left(u_1 \hat{\S}_{11}^{-1/2}, \dots, u_k \hat{\S}_{kk}^{-1/2}\right)^\top(\K \hat{\vartheta} -
\K \vartheta).
\end{eqnarray*}
Since $\hat{\S}$ is a consistent estimate of $\S$ it
follows that $u_j \hat{\S}_{jj}^{-1/2} \cP u_j \S_{jj}^{-1/2}$.
Together with (\ref{dist_f}) we can apply Slutsky's theorem \citep[Theorem 2.3.3
in][]{Lehmann1999} to show that
\begin{eqnarray*}
\sqrt{n} u_j \hat{\S}_{jj}^{-1/2} (\K \hat{\vartheta} - \K \vartheta)_j & \cL &
\N_1\left(0, u_j^2 \S_{jj}^{-1} \S_{jj}\right) \\
& = & \N_1\left(0, u_j^2\right).
\end{eqnarray*}
Because the above holds for every $\uu \in \R^k$, the asymptotic multivariate
normality of $\T$ follows from the Cram{\'e}r-Wold device. 
Based on the statistic $\T$ we will derive test statistics 
and utilize its limiting distribution
as reference distribution when constructing
simultaneous inference procedures in the next section.

\section{Global and Simultaneous Inference} \label{siminf}

Based on the results from Section~\ref{model}, we now focus on the derivation
of suitable inference procedures. We start considering the general linear 
hypothesis \citep{Searle1971}
\begin{eqnarray*}
H_0: \K \vartheta = \m.
\end{eqnarray*}
Under the conditions of $H_0$ it follows from Section~\ref{model} that
$\T = \sqrt{n} \hat{\D} (\K \hat{\vartheta} - \m) \cL \N_k(0, \Cor)$. 
This limiting distribution
will now be used as the reference distribution when constructing the
inference procedures.
The global hypothesis $H_0$ can be tested using standard global tests, such as
the $F$- or the $\chi^2$-test. An alternative approach is to use maximum tests, as explained
in Subsection~\ref{global}. Note that a small global $p$-value
leading to a rejection of $H_0$ does not give further indication about the nature of the
significant result. Therefore, one is often interested in the individual null hypotheses
\begin{eqnarray*}
H_0^j: (\K\vartheta)_j = \m_j.
\end{eqnarray*}
(Note that $H_0 = \bigcap_{j = 1}^k H_0^j$.) Testing the hypotheses set
$\{H_0^1, \ldots, H_0^k\}$ simultaneously thus requires the individual 
assessments while maintaining the
familywise error rate, as discussed in Subsection~\ref{simtest}

At this point it is worth
considering two special cases. A stronger assumption than asymptotic normality
of $\hat{\vartheta}$ (\ref{normality}) is exact normality, i.e.,
$\sqrt{n} (\hat{\vartheta} - \vartheta) \sim \N_p(0, \Sigma)$. 
If the covariance matrix $\Sigma$ is known, it
follows by standard arguments that $\T \sim \N_k(0, \Cor)$.
Otherwise, if $\Sigma = \sigma^2 \A$, where $\A$ is fixed and known
but $\sigma^2$ is an unknown constant (which is the typical situation of linear models
with normal i.i.d.~errors and constant variance), the
exact distribution of $\T$ is a $k$-dimensional multivariate
$t_p(\nu, \Cor)$
distribution with $\nu$ degrees of freedom ($\nu = n - p - 1$ for linear models) 
see \citep{Tong1990}.


\subsection{Global Inference} \label{global}
%\paragraph{Global tests.}

The $F$- and the $\chi^2$-test are classical approaches to assess the global null hypothesis $H_0$.
Standard results ensure that
\begin{eqnarray*}
X^2 & = & (\K \hat{\vartheta} - \m)^\top \hat{\S}^+ (\K \hat{\vartheta} - \m) \cL \chi^2(\Rg(\hat{\S}))
\quad \text{for } \hat{\vartheta} \cL \N_p(\vartheta, \Sigma), \text{ and }  \\
F & = &  \frac{X^2}{\Rg(\hat{\S})} \sim \F(\nu, \Rg(\hat{\S})) \quad \text{for }
\hat{\vartheta} \sim \N_p(\vartheta, \Sigma),
\end{eqnarray*}
where $\Rg(\hat{\S})$ and $\nu$ are the corresponding degrees of freedom and $\hat{\S}^+$
is the Moore-Penrose inverse of $\hat{\S}$.


Another suitable scalar test statistic for testing the global hypothesis
$H_0$ is to consider the maximum of the individual test statistics
$T_1, \dots, T_k$ of the statistic 
$\T = (T_1, \dots, T_k)$, leading to a max-$t$ type test statistic $\max(|\T|)$.
The distribution of this statistic under the conditions of $H_0$
can be handled through the $k$-dimensional distribution
\begin{eqnarray}
\label{maxt}
\Prob(\max(|\T|) \le t)  \cong  \int\limits_{-t}^t \cdots \int\limits_{t}^t
\varphi_k(x_1, \dots, x_k; \Cor, \nu) \, dx_1 \cdots dx_k
\end{eqnarray}
for some $t \in \R$, where $\varphi_k$ is the density function of either the limiting
$k$-dimensional multivariate
normal (with $\nu = \infty$ and the `$\approx$' operator) or the exact multivariate $t_p(\nu,
\Cor)$-distribution (with $\nu < \infty$ and the `$=$' operator).
Since $\Cor$ is usually unknown, we plug-in the consistent estimate
$\hat{\Cor} := \hat{\D} \hat{\S} \hat{\D}^\top$.
The resulting global $p$-value for $H_0$ is $1 - \Prob(\max(|\T|) \le \max|\tt|)$
when $\T = \tt$ has been observed.
Efficient methods
for approximating the above multivariate normal and $t$
probabilities are described in \cite{Genz1992,GenzBretz1999,BretzGenzHothorn2001}
and \cite{GenzBretz2002}.
%The procedures
%are applicable to small and moderate problems with up to $k < 100$ hypotheses.

In contrast to the global $F$- or $\chi^2$-test,
the max-$t$ test $\max(|\T|)$ also provide information,
which of the $k$ individual null hypotheses $H_0^j, j = 1, \dots, k$
is significant as shown in the next subsection.

\subsection{Simultaneous Inference} \label{simtest}
%\paragraph{Simultaneous tests.}

We now consider testing the $k$ null hypotheses $H_0^1, \ldots, H_0^k$ individually and
require that the familywise error rate, i.e., the probability
of falsely rejecting at least one true null hypothesis, is bounded by
the nominal significance level $\alpha \in (0, 1)$. In what follows
we use adjusted $p$-values to describe the decision rules.
Adjusted $p$-values are defined as the
smallest significance level for which one still rejects an
individual hypothesis $H_0^j$, given a particular multiple
test procedure. In the present context, the adjusted
$p$-value for the $j$th individual two-sided hypothesis
$
H_0^j: (\K \vartheta)_j = \m_j, j = 1, \dots, k,
$
is given by
\begin{eqnarray*}
p_j = 1 - \Prob(\max(|\T|) \le |t_j|),
\end{eqnarray*}
where $t_1, \dots, t_k$ denote the observed test statistics.
By construction, we can reject an elementary null hypothesis $H_0^j$, $j= 1, \ldots, k$,
whenever the associated adjusted $p$-value is less than or equal to the pre-specified significance
level $\alpha$, i.e., $p_j \leq \alpha$. The adjusted $p$-values are calculated
from expression~(\ref{maxt}).

Similar results also hold for one-sided testing problems. The adjusted $p$-values for the two different
directions of the alternative hypothesis are given by
\begin{eqnarray*}
& & H_0: \K \vartheta \ge \m \text{ vs. } H_1: \K \vartheta < \m  \quad \Rightarrow \quad p_j  =  1 - \Prob(\max(\T) \le
t_j) \quad \text{("less")}, \\
& & H_0: \K \vartheta \le \m \text{ vs. } H_1: \K \vartheta > \m \quad \Rightarrow \quad p_j  =  1 - \Prob(\min(\T) \ge
t_j) \quad \text{("greater")}.
\end{eqnarray*}
As before, these probabilities can be calculated from
\begin{eqnarray*}
& & \Prob(\max(\T) \le t)  \cong\int\limits_{-\infty}^t \cdots \int\limits_{-\infty}^t
\varphi_k(x_1, \dots, x_k; \nu, \Cor) \, dx_1 \cdots dx_k \quad \text{("less")},\\
& & \Prob(\min(\T) \ge t) \cong  \int\limits_{t}^\infty \cdots \int\limits_{t}^\infty \varphi_k(x_1, \dots, x_k;
\nu, \Cor) \, dx_1 \cdots dx_k \quad
\text{("greater")}.
\end{eqnarray*}
Again, we refer to \cite{Genz1992,GenzBretz1999,BretzGenzHothorn2001}
and \cite{GenzBretz2002} for the numerical details.

%\paragraph{Simultaneous confidence intervals.}

In addition to a simultaneous test procedure, a simultaneous $(1 - 2\alpha) \times 100\%$
confidence interval for $\K \vartheta$ is given by
\begin{eqnarray*}
\K \hat{\vartheta} \pm q_\alpha \text{diag}(\hat{\S})^{-1/2}
\end{eqnarray*}
where $q_\alpha$ is the $1 - \alpha$
quantile of the distribution of $\T$
such that $\Prob(\max{|\T|} \le q_\alpha) \ge 1 - \alpha$. The corresponding
one-sided versions are defined analogously.


It should be noted that the simultaneous inference procedures described so far
belong to the class of single-step procedures, since a common critical value is
used for the individual tests. However, single-step procedures
can always be improved by stepwise extensions based on the closed test procedure.
That is, for a given family of null
hypotheses $H_0^1, \dots, H_0^k$, an individual hypothesis $H_0^j$ is
rejected only if all intersection hypotheses $H_J = \bigcap_{i \in
J} H_0^i$ with $j \in J \subseteq \{1, \dots, k\}$ are rejected
\citep{Marcusetal1976}.
Such stepwise extensions can thus be applied to any of the methods
discussed in this paper, see for example \cite{Westfall1997} and
\cite{WestfallTobias2007}.
%%In fact, the \Rpackage{multcomp} package
%%introduced in Section~\ref{implementation} uses max-$t$
%%type statistics for each intersection hypothesis based on the
%%methods from this paper, thus accounting for stochastic
%%dependencies. Furthermore, the implementation of \Rpackage{multcomp}
%%exploits logical constraints, leading to computationally
%%efficient, yet powerful truncated closed test procedures, see
%%\cite{Westfall1997} and \cite{WestfallTobias2007}.

\section{Applications} \label{applications}

The methodological framework described in Sections~\ref{model} and
\ref{siminf} is
very general and thus applicable to a wide range of statistical
models. Many estimation techniques, such as (restricted) maximum
likelihood and M estimates, provide at least asymptotically normal
estimates of the parameters together with consistent estimate of
the covariance matrix. In this section we
illustrate the generality of the methodology by reviewing some
potential applications. Detailed numerical examples are discussed
in Section~\ref{illustrations}. In what follows, we assume $\m =
0$ only for the sake of simplicity.
The next paragraphs highlight a subjective selection of
some special cases of practical importance.

\paragraph{Multiple Linear Regression.}

In standard regression models the observations $\Z_i$ of subject
$i=1, \ldots, n$ consist of a response variable $Y_i$ and a
vector of covariates $\X_i = (X_{i1}, \dots, X_{iq})$, such that
$\Z_i = (Y_i, \X_i)$ and $p = q + 1$. The response is modelled by a linear
combination of the covariates with normal error $\varepsilon_i$ and
constant variance $\sigma^2$,
\begin{eqnarray*}
Y_i = \beta_0 + \sum_{i = 1}^q \beta_i X_{ij} + \sigma \varepsilon_i,
\end{eqnarray*}
where $\varepsilon = (\varepsilon_1, \dots, \varepsilon_n)^\top \sim \N_n(0, \mathbf{I}_n).$
The parameter vector of interest is $\vartheta = (\beta_0, \beta_1, \dots, \beta_q)$,
which is usually estimated by
\begin{eqnarray*}
\hat{\vartheta} = \left(\X^\top\X\right)^{-1} \X^\top \Y
\sim \N_{q+1}\left(\vartheta, \sigma^2 \left(\X^\top\X\right)^{-1}\right),
\end{eqnarray*}
where $\Y = (Y_1, \dots, Y_n)$ denotes the response vector and $\X
= (1, (X_{ij}))_{ij}$ denotes the design matrix, $i = 1, \dots, n,
j = 1, \dots, q$. Thus, for every matrix $\K \in \R^{k,q+1}$ of
constants determining the experimental questions of interest we
have
\begin{eqnarray*}
\K \hat{\vartheta} \sim \N_k(\K \vartheta, \sigma^2 \K \left(\X^\top\X\right)^{-1} \K^\top).
\end{eqnarray*}
Under the null hypothesis $\K \vartheta = 0$ the standardized test statistics
\begin{eqnarray*}
\T = \hat{\D} \K \hat{\vartheta} \sim t_{q+1}(n - q, \Cor),
\end{eqnarray*}
where $\hat{\D}$ is the diagonal matrix of the inverse estimated
standard deviations of $\K \hat{\vartheta}$ and $\Cor$ is the correlation matrix
as given in Section~\ref{siminf}. The body fat prediction example
presented in Subsection \ref{bodyfat} illustrates the application of simultaneous
inference procedures in the context of variable selection in linear regression models.

\paragraph{One-way ANOVA.}

Consider a one-way ANOVA model for a factor measured at $q$ levels
with a continuous response
\begin{eqnarray}\label{one-way}
Y_{ij} = \mu + \gamma_{j} + \varepsilon_{ij}
\end{eqnarray}
and independent normal errors $\varepsilon_{ij} \sim \N_1(0, \sigma^2), j = 1, \dots, q, i = 1, \dots, n_j$.
Note that the model description in (\ref{one-way}) is overparameterized.
A standard approach is to consider a suitable re-parametrization.
The so-called "treatment contrast" vector
$\vartheta = (\mu, \gamma_2 - \gamma_1, \gamma_3 - \gamma_1, \dots, \gamma_q - \gamma_1)$
is, for example, the default re-parametrization used in \RR.

Many classical multiple comparison procedures can be embedded into this framework,
including Dunnett's many-to-one comparisons and
Tukey's all-pairwise differences. For Dunnett's procedures,
the differences $\gamma_i - \gamma_1$ are tested for all $i=2, \ldots, i$, where $\gamma_1$ denotes the mean treatment effect of a control group.
In the notation from Section~\ref{model} we thus have
\begin{eqnarray*}
\K_\text{Dunnett} = (0, \diag(q))
\end{eqnarray*}
resulting in the linear functions
\begin{eqnarray*}
\K_\text{Dunnett} \vartheta = (\gamma_2 - \gamma_1, \gamma_3 - \gamma_1, \dots, \gamma_q - \gamma_1)
\end{eqnarray*}
of interest.
For Tukey's procedure, the interest is in all-pairwise comparisons of the parameters $\gamma_1, \dots, \gamma_q$. For $q = 3$, for example, we have
\begin{eqnarray*}
\K_\text{Tukey} = \left(
\begin{array}{rrr} 0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 1 & -1
\end{array} \right)
\end{eqnarray*}
with
\begin{eqnarray*}
\K_\text{Tukey} \vartheta = (\gamma_2 - \gamma_1, \gamma_3 - \gamma_1, \gamma_2 - \gamma_3).
\end{eqnarray*}

Many further multiple comparison procedures have been investigated in the past, which all fit into this framework. We refer to Bretz et al. (2001) for a related comprehensive list.
Note that under the standard ANOVA assumptions of i.i.d. normal errors with constant
variance the vector of test statistics $\T$ follows a multivariate $t$ distribution.
Thus, related simultaneous tests and confidence intervals do not rely on asymptotics and can be computed analytically instead, as shown in Section~\ref{siminf}.

To illustrate simultaneous inference procedures in one-way ANOVA models, we
consider all pairwise comparisons of expression levels for various genetic conditions
of alcoholism in Subsection~\ref{alpha}.

\paragraph{Further parametric models.}

In \emph{generalized linear models}, the exact distribution of the parameter estimates
is usually unknown and thus the asymptotic normal distribution is the basis for all
inference procedures. When we are interested in inference about model
parameters corresponding to levels of a certain factor, the same multiple
comparison procedures as sketched above are available.
\emph{Linear and non-linear mixed effects} models fitted by restricted
maximum-likelihood provide
the data analyst with asymptotically normal errors and a consistent
covariance matrix as well so that all assumptions of our framework
are met and one can set up simultaneous inference procedures
for these models as well.
The same is true for the \emph{Cox model} or other parametric survival models
such as the \emph{Weibull survival model}.

We use logistic regression models to estimated the probability of suffering
from Alzheimer's disease in Subsection~\ref{alzheimer}, compare several risk
factors for survival of leukemia patients by means of a Weibull model in
Subsection~\ref{AML} and obtain probability estimates of deer browsing for
various tree species from mixed models in Subsection~\ref{forest}.

\paragraph{Robust simultaneous inference.}

Yet another application is to use robust variants of the previously discussed statistical models.
One possibility is to consider the use
of sandwich estimators $\hat{\Sigma}$ for the covariance matrix $\Sigma$ when, for example,
the variance homogeneity assumption is violated.
An alternative is to apply robust estimation techniques in 
linear models, for example S-, M- or MM-estimation \citep[see][for
example]{RousseeuwLeroy2003, mfluc:Stefanski+Boos:2002, Yohai1987, mfluc:White:1994}, 
which again provide us with asymptotically normal estimates.

The reader is referred to Subsection~\ref{bodyfat} for some numerical 
examples illustrating these ideas.


\section{Implementation} \label{implementation}

The \Rpackage{multcomp} package \citep{pkg:multcomp} in \RR{} \citep{R2007} provides
a general implementation of the framework for simultaneous inference in
(semi-)\-parametric models described in Sections~\ref{model} and~\ref{siminf}.
The numerical examples in Section~\ref{illustrations}
will all be analyzed using the \Rpackage{multcomp} package. In this section 
we briefly introduce the
\Rpackage{multcomp} package and refer the reader to the online documentation 
of the package for the technical details.

Estimated model coefficients $\hat{\vartheta}$ and their covariance
matrix $\hat{\Sigma}$ are accessible in \RR{} with \Rcmd{coef()}
and \Rcmd{vcov()} methods available for most statistical models in \RR, such as
objects of class \Rclass{lm},
\Rclass{glm}, \Rclass{coxph}, \Rclass{nlme}, or \Rclass{survreg}.
Having this information, the \Rcmd{glht()} function sets up the \underline{g}eneral
\underline{l}inear \underline{h}ypo\underline{t}hesis for a model
`\Robject{model}' and a representation of the matrix $\K$ (via its
\Robject{linfct} argument):
\begin{Sinput}
glht(model, linfct, alternative = c("two.sided", "less", "greater"),
     rhs = 0, ...)
\end{Sinput}
The two remaining arguments
\Rarg{alternative} and \Rarg{rhs} define the direction
of the alternative (see Section~\ref{siminf}) and $\m$, respectively.

The matrix $\K$ can be described in three different ways:
\begin{itemize}
\item by a matrix with \Rcmd{length(coef(model))} columns, or
\item by an expression or character vector giving a symbolic description
      of the linear functions of interest, or
\item by an object of class \Rclass{mcp}
      (for \underline{m}ultiple \underline{c}omparison \underline{p}rocedure).
\end{itemize}
The last alternative is convenient when contrasts of factor levels are
to be compared and the model contrasts used to define the design matrix
of the model have to be taken into account. The \Rcmd{mcp()} function
takes the name of the factor to be tested as an argument as well as a character
defining the type of comparisons as its value. For example,
\Rcmd{mcp(type = "Tukey")} sets up a matrix $\K$ for Tukey's all-pairwise
differences among the levels of the factor \Robject{type},
which has to appear on right hand side of the model formula
of \Robject{model}. In this particular case, we need to assume
that \Rcmd{model.frame()} and \Rcmd{model.matrix()} methods for
\Robject{model} are available as well.

Objects of class \Rclass{glht} returned by \Rcmd{glht()} include
\Rcmd{coef()} and \Rcmd{vcov()} methods to compute
$\K \hat{\vartheta}$ and $\hat{\S}$. Furthermore, a \Rcmd{summary()} method
is available to perform different tests (max $t$, $\chi^2$ and
$F$-tests) and $p$-value adjustments, including those taking
logical constraints into account \citep{Shaffer1986, Westfall1997}.
In addition, the \Rcmd{confint()} method applied to objects of class
\Rclass{glht} returns simultaneous confidence intervals and allows
for a graphical representation of the results. The numerical 
accuracy of adjusted $p$-values and
simultaneous confidence intervals implemented in \Rpackage{multcomp}
is continuously checked against results reported by
\cite{Westfall1999}.

\input{illustrations}

\input{trees}

\section{Conclusion}

Multiple comparisons in linear models have been in use for a long time,
see \cite{HochbergTamhane1987}, \cite{Hsu1996}, and \cite{Bretzetal2008}.
In this paper we have extended the theory to a broader class
of parametric and semi-parametric statistical models, which allows a unified treatise of
multiple comparisons and other simultaneous inference procedures 
in generalized linear models, mixed models, survival models, robust models etc.
In essence, all what is required is a parameter estimate
$\hat{\vartheta}$ following an asymptotic multivariate normal distribution,
and a consistent estimate $\hat{\Sigma}$ of its covariance matrix.
Standard software packages can be used to compute these quantities.
As shown in this paper, these quantities are sufficient to derive
powerful simultaneous inference procedures, which are tailored to the 
experimental questions under investigation. Therefore, honest decisions
based on simultaenous inference procedures maintaining a pre-specified 
family-wise error rate can now be based on almost all classical
and modern statistical models.

The examples presented in Section~\ref{illustrations} illustrate two facts.
At first, the presented approach helps to formulate simultaneous
inference procedures in situations that were previously hard to deal
with and, at second, a flexible open-source implementation offers tools
to actually perform such procedures rather easily. With the
\Rpackage{multcomp} package, freely available from
\url{http://CRAN.R-project.org}, honest simultaneous inference 
is only two commands away.
\marginpar{vignette}

\bibliographystyle{biometrika}
\bibliography{references}

\end{document}
