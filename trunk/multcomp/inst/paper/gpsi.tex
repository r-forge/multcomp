
\documentclass[12pt]{article}
\usepackage{a4wide}
\usepackage[lists,heads]{endfloat}

\input{header}

\hypersetup{%
  pdftitle = {Generalized Simultaneous Inference in Parametric Models},
  pdfsubject = {Manuscript},
  pdfauthor = {Torsten Hothorn and Frank Bretz and Peter Westfall},
%% change colorlinks to false for pretty printing
  colorlinks = {true},
  linkcolor = {blue}, 
  citecolor = {blue}, 
  urlcolor = {red},   
  hyperindex = {true},
  linktocpage = {true},
}
 
\title{Generalized Simultaneous Inference \\ in Parametric Models}
\author{\textbf{Torsten Hothorn} \\
Institut f{\"u}r Statistik \\
Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen \\
Ludwigstra{\ss}e 33, D--80539 M{\"u}nchen, Germany\\
\and \textbf{Frank Bretz} \\
Statistical Methodology, Clinical Information Sciences\\
Novartis Pharma AG \\
CH-4002 Basel, Switzerland \\
\and \textbf{Peter Westfall} \\
Texas Tech University \\
Lubbock, TX 79409, U.S.A}

%% ToDo

%% Introduction: more elaborate formulations
%% add some references to recently published technology

%% Section 2  / 3: Model & Inference
%% check
%% reference for multivariate T distribution and
%% its applications

%% Section 4: Applications
%% references for M, S, REML etc. estimation

%% Section 5: Implementation
%% check

%% Section 6: Illustrations:
%% check

%% Section 7: Discussion
%% some comments on why we saved the world today.
%% maybe some more references

\begin{document}


\maketitle
\thispagestyle{empty}
\setcounter{page}{0}

\begin{abstract}
Abstract
\end{abstract}

\textbf{Key words}: multiple tests, multiple comparisons, simultaneous 
confidence intervals,
adjusted $p$-values, multivariate normal distribution, robust statistics.

\newpage

\section{Introduction}

Multiplicity is an intrinsic problem of any simultaneous inference. 
If each of $k$, say, null hypotheses is tested at nominal level $\alpha$, 
the overall type I error rate can be substantially larger than $\alpha$.
That is, the probability of at least one erroneous rejection is larger than $\alpha$ for 
$k \geq 2$. Common multiple testing and comparison procedures adjust for multiplicity and thus 
ensure that the overall type I error remains below the pre-specified significance level.
Example of such multiple comparison procedures include Dunnett's many-to-one comparisons,
Tukey's all-pairwise differences, sequential pairwise contrasts, comparisons with the 
average, changepoint analyses, dose-response contrasts, etc. These procedures are
all well established for classical regression and ANOVA models allowing for covariates 
and/or factorial treatment structures with i.i.d.~normal errors and constant 
variance, see ??? and the references therein. For a general reading 
on multiple comparison procedures we refer to 
\cite{HochbergTamhane1987} and \cite{Hsu1996}.

In this paper we aim at a unified description of simultaneous
inference procedures in parametric models with generally correlated parameter estimates.
Each individual null hypothesis is specified through a linear
combination of the model parameters and we allow
for $k$ of such null hypotheses to be tested simultaneously.
The general framework described here extends the current canonical theory 
with respect to the following aspects: (i) model assumptions, such as normality 
and homoscedasticity
are relaxed, thus allowing for simultaneous inference
in generalized linear models, mixed-effects models,
survival models, etc.; (ii) arbitrary linear functions of the
parameters are allowed, not just contrasts of means in AN(C)OVA models; (iii)
computing the reference distribution is feasible for arbitrary designs,
especially unbalanced designs; and (iv)
a unified implementation is provided which allows for a fast transition of the
theoretical results to the desks of data analysts interested
in simultaneous inferences for multiple hypotheses.

Accordingly, the paper is organized as follows. 
The first Section defines the general model and obtains 
the asymptotic or exact distribution of linear functions
of model parameters under rather weak conditions. 
In Section~\ref{siminf} we
describe the framework for simultaneous inference
procedures in general parametric models. An overview about important applications
of the methodology is given in Section~\ref{applications} followed
by a short discussion of the software implementation in Section~\ref{implementation}.
Most interesting from a practical point of view is
Section~\ref{illustrations} where we analyze four rather challenging
problems with the tools developed in this paper.


\section{Model and Estimation} \label{model}

Let $\M(\Z_n, \vartheta, \eta)$ denote a (semi-)parametric statistical model.
The vector of $n$ observations is described by $\Z_n = (\Z_1, \dots, \Z_n)$. 
The model is equiped with fixed
but unknown parameters of interest $\vartheta \in \R^p$ 
and other (random or nuisance) parameters $\eta$. 
We are primarily interested in the linear
functions $f_{\K, \m}(\vartheta) = \K \vartheta - \m$
of the parameter vector $\vartheta$ as specified through the finite
constants $\K \in \R^{k, p}$
and $\m \in \R^k$. Furthermore, we are given
an estimate $\hat{\vartheta} \in \R^p$ of the parameter
vector $\vartheta$. In this section we describe the underlying model
assumptions and derive the limiting distribution of $f_{\K,\m}(\hat{\vartheta})$
%% FB: Satz wegen Kontrasten.


\paragraph{Model assumptions.}

We assume that the parameter estimates $\hat{\vartheta}$ converge in law
to a $p$-dimensional multivariate normal distribution as the sample
size $n$ tends to infinity:
\begin{eqnarray} \label{normality}
\hat{\vartheta} \cL \N_p\left(\vartheta, \Sigma\right)
\end{eqnarray}
with constant and finite covariance matrix $\Sigma \in \R^{p,p}$.
Since the covariance matrix is usually unknown, we assume 
that we are given a consistent estimate $\hat{\Sigma} \cP \Sigma$.

Because we only assume asymptotic normality of the parameter estimates
and consistent information about their covariance matrix,
a huge class of classical and modern statistical models, from the linear
regression model and ANOVA models to generalised linear models, linear mixed
effects models, the Cox model, robust linear models etc.~pp., are 
covered. Standard software packages can be used to fit such models
and obtain estimates $\hat{\vartheta}$ and $\hat{\Sigma}$ which
are the only two quantities that are needed for what follows.
It should be noted that the model parameters are not necessarily
interpreted as means or differences of means in AN(C)OVA models
and thus not only simultaneous contrasts of such means, the corresponding
inference procedures are known as \emph{multiple comparisons}, 
are under test here. Therefore we use the more general term
\emph{simultaneous inference} in Section~\ref{siminf}.


\paragraph{Limiting distribution of $f_{\K,\m}(\hat{\vartheta})$.}

The linear function $f_{\K, \m}$ is continuous and differentiable in
$\vartheta$ with Jacobian $\K$ and thus, by Cram{\'e}r's Theorem, 
the linear function $f$ of the parameter estimates
converges in law to a (possibly singular) $k$-dimensional normal distribution:
\begin{eqnarray} \label{dist_f}
f_{\K, \m}(\hat{\vartheta}) = \K \hat{\vartheta} - \m 
\cL \N_k\left(\K \vartheta - \m, \K \Sigma \K^\top\right).
\end{eqnarray}
Furthermore, because the quadratic form $\K \hat{\Sigma} \K^\top$
is continuous and, by assumption, $\hat{\Sigma} \cP \Sigma$, 
it follows from the continuous mapping theorem that we can 
consistently estimate the unknown covariance matrix $\K \Sigma \K^\top$ utilizing
the consistent estimate $\hat{\Sigma}$ of the covariance matrix $\Sigma$:
\begin{eqnarray*}
\hat{\S} := \K \hat{\Sigma} \K^\top \cP \K \Sigma \K^\top =: \S.
\end{eqnarray*}

\paragraph{A multivariate statistic and its limiting distribution.}

Let $\T := \hat{\D} (\K \hat{\vartheta} - \m) \in \R^k$ denote
the $k$-dimensional standardized test statistic where $\hat{\D}$ is the diagonal
matrix of standard deviations
\begin{eqnarray*}
\hat{\D} := \text{diag}\left(\text{diag}(\hat{\S})^{-1/2}\right) \cP 
\text{diag}\left(\text{diag}(\S)^{-1/2}\right) =: \D \in \R^{k,k}
\end{eqnarray*}
In order to show that also $\T$ converges in law to a $k$-dimensional multivariate
normal distribution
\begin{eqnarray*}
\T \cL \N_k\left(\D(\K \vartheta - \m), \Cor := \D \S \D^\top\right)
\end{eqnarray*}
we utilize the Cram{\'e}r-Wold device and show that for some $\uu \in \R^k$ 
the linear function $\uu^\top \T$ is univariate normal. Consider
\begin{eqnarray*}
\uu^\top \T & = & \uu^\top \hat{\D} (\K \hat{\vartheta} - \m) \\
& = & \left(u_1 \hat{\S}_{11}^{-1/2}, \dots, u_k \hat{\S}_{kk}^{-1/2}\right)^\top(\K \hat{\vartheta} - \m).
\end{eqnarray*}
Since $\hat{\S}$ is a consistent estimate of $\S$ it 
is follows that $u_j \hat{\S}_{jj}^{-1/2} \cP u_j \S_{jj}^{-1/2}$.
Together with (\ref{dist_f}) we can apply Slutsky's theorem to show that
\begin{eqnarray*}
u_j \hat{\S}_{jj}^{-1/2} (\K \hat{\vartheta} - \m)_j & \cL &  
\N_1\left(u_j \S_{jj}^{-1/2} (\K \vartheta - \m)_j, u_j^2 \S_{jj}^{-1} \S_{jj}\right) \\
& = & \N_1\left(u_j \S_{jj}^{-1/2} (\K \vartheta - \m)_j, u_j^2\right)
\end{eqnarray*}
Because the above holds for every $\uu \in \R^k$, the asymptotic multivariate
normality of $\T$ follows from the Cram{\'e}r-Wold device. 


\section{Global and Simultaneous Inference} \label{siminf}

Since we are interested in inference, both tests and confidence
intervals, about linear functions $\K \vartheta$ of the parameter vector
$\vartheta$ we now consider the general linear hypothesis \citep{Searle1971}
\begin{eqnarray*}
H_0: \K \vartheta = \m.
\end{eqnarray*}
Under the conditions of $H_0$ it follows from Section~\ref{model} that 
$\T \cL \N_k(0, \Cor)$ holds true. This limiting distribution
will now be used as the reference distribution when constructing 
inference procedures.
The global hypothesis $H_0$ can be tested using classical
global $F$- or $\chi^2$ tests or the maximum-test to be explained
in the next paragraph. However, a small global $p$-value
leading to a rejection of $H_0$ doesn't tell us which subset of the
 $k$ partial hypotheses defined by the
$j$th row of the matrix $\K$
\begin{eqnarray*}
H_0^j: (\K\vartheta)_j = \m_j \text{ with } H_0 = \bigcap_{j = 1}^k H_0^j
\end{eqnarray*}
actually caused our rejection of $H_0$. Instead, we are interested
in $p$-values for each of the $p$ partial hypotheses $H_0^j$ or
confidence intervals for the parameters $(\K\vartheta)_j$ which
maintain the family-wise error rate.

At this point it is worth
considering two special cases. A stronger assumption than asymptotic normality
of $\hat{\vartheta}$ (\ref{normality}) is exact normality, i.e., 
$\hat{\vartheta} \sim \N_p(\vartheta, \Sigma)$. Under the unrealistic
further assumption of the covariance matrix $\Sigma$ being known, it
follows by standard arguments that $\T \sim \N_k(0, \Cor)$. 
When $\Sigma = \sigma^2 \A$ with $\A$ fix and known 
but $\sigma^2$ is not given (the typical situation of linear models
with normal iid errors and constant variance), the
exact distribution of $\T$ is a $k$-dimensional multivariate
$t_p(\nu, \Cor)$
distribution with $\nu$ degrees of freedom ($\nu = n - p - 1$ for linear models).
\marginpar{FB: Ref}

\paragraph{Global tests.}

It is well-known that aquadratic form based on the Moore-Penrose inverse $\hat{\S}^+$ of 
$\hat{\S}$ or the usual $F$-test (assuming exact normality of $\hat{\vartheta}$) 
can be utilized to construct a global test for testing $H_0$:
\begin{eqnarray*}
X^2 & = & (\K \hat{\vartheta} - \m)^\top \hat{\S}^+ (\K \hat{\vartheta} - \m) \cL \chi^2(\Rg(\hat{\S})) 
\quad \text{for } \hat{\vartheta} \cL \N_p(\vartheta, \Sigma) \\
F & = &  \frac{X^2}{\Rg(\hat{\S})} \sim \F(\nu, \Rg(\hat{\S})) \quad \text{for } 
\hat{\vartheta} \sim \N_p(\vartheta, \Sigma)
\end{eqnarray*}
where $\Rg(\hat{\S})$ and $\nu$ are the corresponding degrees of freedom.

Another suitable scalar test statistic for testing the global hypothesis 
$H_0$ is $\max(|\T|)$ leading to the so-called \emph{maximum-test}. 
The distribution of this statistic under the conditions of $H_0$
can be calculated or approximated
by evaluating the $k$-dimensional exact or limiting distributions for some $t \in R$:
\begin{eqnarray*}
\Prob(\max(|\T|) \le t)  \cong  \int\limits_{-t}^t \cdots \int\limits_{t}^t 
\varphi_k(x_1, \dots, x_k; \Cor, \nu) \, dx_1 \cdots dx_k 
\end{eqnarray*}
where $\varphi_k$ is either the density function of the limiting $k$
dimensional multivariate
normal (with $\nu = \infty$ and `$\approx$') or exact multivariate $t_p(\nu,
\Cor)$-distribution (`='). 
Since $\Cor$ is usually unknown, we plug-in a consistent estimate
$\hat{\Cor} := \hat{\D} \hat{\S} \hat{\D}^\top$. Efficient methods
for approximating the above multivariate normal and $t$ 
probabilities are described in \cite{Genz1992,GenzBretz1999,BretzGenzHothorn2001}
and \cite{GenzBretz2002}. The procedures
are applicable to small and moderate problems with up to $k < 100$ hypotheses.
The global $p$-value for $H_0$ is $1 - \Prob(\max(|\T|) \le \max|\tt|)$.

In contrast to the global $F$- or $\chi^2$ tests, 
the maximum-test $\max(|\T|)$ can shed light 
on the question which of the $k$ partial hypotheses $H_0^j, j = 1, \dots, k$ 
lead to the rejection of the global hypothesis $H_0$ as will be shown in the
next paragraph. 

\paragraph{Simultaneous tests.}

We now obtain one $p$-value for each of the $k$ partial hypotheses and
require that the family-wise error rate, i.e., the probability
of falsely rejecting at least one true hypothesis, is not larger than
the nominal level $\alpha \in (0, 1)$. A single-step adjusted two-sided 
$p$-value for the $j$th partial hypothesis 
\begin{eqnarray*}
H_0^j: (\K \vartheta)_j = \m_j \quad j = 1, \dots, k.
\end{eqnarray*}
is given by $p_j = 1 - \Prob(\max(|\T|) \le |t_j|)$ when the test statistic 
$\tt  = (t_1, \dots, t_k)$
has been observed. 

For one-sided problems, the adjusted $p$-values for two different
directions of the alternative $H_1$ are given as follows:
\begin{eqnarray*}
& & H_0: \K \vartheta \ge \m \text{ vs. } H_1: \K \vartheta < \m  \quad \Rightarrow \quad p_j  =  1 - \Prob(\max(\T) \le
t_j) \quad \text{("less")} \\
& & H_0: \K \vartheta \le \m \text{ vs. } H_1: \K \vartheta > \m \quad \Rightarrow \quad p_j  =  1 - \Prob(\min(\T) \ge
t_j) \quad \text{("greater")}.
\end{eqnarray*}
These probabilities can be calculated as 
\begin{eqnarray*}
& & \Prob(\max(\T) \le t)  \cong\int\limits_{-\infty}^t \cdots \int\limits_{-\infty}^t 
\varphi_k(x_1, \dots, x_k; \nu, \Cor) \, dx_1 \cdots dx_k \quad \text{("less")}\\
& & \Prob(\min(\T) \ge t) \cong  \int\limits_{t}^\infty \cdots \int\limits_{t}^\infty \varphi_k(x_1, \dots, x_k;
\nu, \Cor) \, dx_1 \cdots dx_k \quad
\text{("greater")}.
\end{eqnarray*}


\paragraph{Simultaneous confidence intervals.}

A simultaneous $(1 - 2\alpha) \times 100\%$ 
confidence interval for $\K \vartheta$ is given by
\begin{eqnarray*} 
\K \hat{\vartheta} \pm q_\alpha \text{diag}(\hat{\S})^{-1/2}
\end{eqnarray*}
where $q_\alpha$ is the $1 - \alpha$ 
quantile of the distribution of $\T$ 
such that $\Prob(\max{|\T|} \le q_\alpha) \ge 1 - \alpha$. The corresponding
one-sided version are defined analogeously.

\section{Applications} \label{applications}

The framework described above is rather general and applicable 
to a wide range of models since most estimation techniques provide us with 
at least asymptotically normal estimates of the parameters 
and a consistent estimate of their covariance matrix. Most prominent
representatives are models fitted by maximum likelihood, M-estimation,
restricted maximum likelihood etc. 
\marginpar{more + Refs}
It should be noted that we didn't make assumptions on the form
of $\K$ whatsoever and thus a rather straightforward construction 
principle for new simultaneous inference procedures is available. In
what follows, we assume $\m = 0$ only for the sake of simplicity.
The next paragraphs highlight a subjective selection of 
some special cases of practical importance.

\paragraph{Multiple Linear Regression.}

In standard regression models the observations $\Z_i$ of subject
$i=1, \ldots, n$ consist of a response variable $Y_i$ and a
vector of covariates $\X_i = (X_{i1}, \dots, X_{iq})$, such that
$\Z_i = (Y_i, \X_i)$ and $p = q + 1$. The response is modelled by a linear
combination of the covariates with normal error $\varepsilon_i$ and
constant variance $\sigma^2$:
\begin{eqnarray*}
Y_i = \beta_0 + \sum_{i = 1}^q \beta_i X_{ij} + \sigma \varepsilon_i \quad 
      \varepsilon = (\varepsilon_1, \dots, \varepsilon_n) \sim \N_n(0, \mathbf{I}_n).
\end{eqnarray*}
The parameter vector of interest is $\vartheta = (\beta_0, \beta_1, \dots, \beta_q)$
usually estimated by 
\begin{eqnarray*}
\hat{\vartheta} = \left(\X^\top\X\right)^{-1} \X^\top \Y 
\sim \N_{q+1}\left(\vartheta, \sigma^2 \left(\X^\top\X\right)^{-1}\right)
\end{eqnarray*}
with design matrix $\X = (1, (X_{ij}))_{ij}, i = 1, \dots, n, j = 1, \dots, q$ and
vector of responses $\Y = (Y_1, \dots, Y_n)$. 
Thus, for every matrix $\K \in \R^{k,q+1}$ of   
constants determining the experimental questions of interest we   
have
\begin{eqnarray*}
\K \hat{\vartheta} \sim \N_k(\K \vartheta, \sigma^2 \K \left(\X^\top\X\right)^{-1}
\K^\top)
\end{eqnarray*}
and under the hypothesis $\K \vartheta = 0$ the standardized test statistics
\begin{eqnarray*}
\T = \hat{\D} \K \hat{\vartheta} \sim \T_{n - q}(0, \Cor)
\end{eqnarray*}
where $\hat{\D}$ is the diagonal matrix of the inverse estimated 
standard deviations of $\K \hat{\vartheta}$ and $\Cor$ is the correlation matrix
as given in Section~\ref{siminf}. The body fat prediction example
presented in Subsection \ref{bodyfat} illustrates the use of simultenous
inference procedures for variable selection.

\paragraph{One-way ANOVA.}

Consider a one-way ANOVA model for a factor measured at $q$ levels
with a continuous response and independent 
normal errors $\varepsilon_{ij} \sim \N_1(0, \sigma^2)$, 
i.e., the model
\begin{eqnarray*}
Y_{ij} = \mu + \gamma_{j} + \varepsilon_{ij}; \quad j = 1,\dots,q; \quad i = 1, \dots,
n_j
\end{eqnarray*}
The model is overparameterized and thus only the parameters
$\vartheta = (\mu, \gamma_2 - \gamma_1, \gamma_3 - \gamma_1, \dots, \gamma_q -
\gamma_1)$ 
are  estimated by classical least-squares. (Note that other contrasts can be used
but theses so-called `treatment contrasts' are the most prominent ones and,
for example, the default in \RR.) 
Most of the classical approaches
to multiple comparisons can be embedded into this simple framework and 
the two most prominent ones are Dunnett's many-to-one procedure and
Tukey's honest significant differences. For Dunnett's procedures,
the differences of all coefficients with $\gamma_1$ are under test and
describe the mean treatment effect of a control group, i.e.,
a linear function
\begin{eqnarray*}
\K_\text{Dunnett} = (0, \diag(q))
\end{eqnarray*}
resulting in the linear functions
\begin{eqnarray*}
\K_\text{Dunnett} \vartheta = (\gamma_2 - \gamma_1, \gamma_3 - \gamma_1, \dots,
\gamma_q - \gamma_1).
\end{eqnarray*}
For Tukey's honest significant differences the interest is in inference on
all possible differences of the model parameters $\gamma_1, \dots, \gamma_q$, i.e., 
the matrix (here for $q = 3$)
\begin{eqnarray*}
\K_\text{Tukey} = \left( 
\begin{array}{rrr} 0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 1 & -1 
\end{array} \right)
\end{eqnarray*}
with $\K_\text{Tukey} \vartheta = (\gamma_2 - \gamma_1, \gamma_3 - \gamma_1,
\gamma_2 - \gamma_3)$.

Many further multiple comparison procedures have been investigated in the past, 
which all fit into this framework. We refer to \cite{BretzGenzHothorn2001} for a related 
comprehensive list. Note that under the standard ANOVA assumptions of
i.i.d.~normal errors with constant variance the vector of test statistics $\T$ 
follows a multivariate $t_k$ distribution. Thus, related simultaneous tests and 
confidence intervals do not rely on asymptotics and can be computed analytically 
instead, as shown in Section~\ref{siminf}.

All-pair comparisons of expression levels for various genetic conditions 
based on an ANOVA model are illustrated in Subsection~\ref{alpha}.

\paragraph{Further parametric models.}

In \emph{generalized linear models}, the exact distribution of the parameter estimates
is usually unknown and thus the asymptotic normal distribution is the basis for all
inference procedures. When we are interested in inference about model
parameters corresponding to levels of a certain factor, the same multiple 
comparison procedures as sketched above are available. 
\emph{Linear and non-linear mixed effects} models fitted by restricted
maximum-likelihood provide
the data analyst with asymptotically normal errors and a consistent
covariance matrix as well so that all assumptions of our framework
are met and one can set up simultaneous inference procedures
for these models as well.
The same is true for the \emph{Cox model} or other parametric survival models
such as the Weibull model.

We use logistic regression models to estimated the probability of suffering
from Alzheimer's disease in Subsection~\ref{alzheimer}, compare several risk
factors for survival of leukemia patients by means of a Weibull model in
Subsection~\ref{AML} and obtain probability estimates of deer browsing for
various tree species from mixed models in Subsection~\ref{forest}.

\paragraph{Robust simultaneous inference.}

An interesting possibility is to move to robust variants of the above
mentioned models. One obvious possibility is the application
of sandwich estimators $\hat{\Sigma}$ of the covariance matrix $\Sigma$, 
for example when the variance homogeneity in linear models is violated.
An even more robust approach is to use robust linear models, for example
fitted using MM.

The reader is referred to Section~\ref{illustrations} for examples of
robustified simultaneous inference.

\section{Implementation} \label{implementation}

The general framework for simultaneous inference in 
(semi-)parametric models described in Section~\ref{model}
opens up the way to a unified implementation of simultaneous inference
procedures for arbitrary models meeting the model assumptions 
(\ref{normality}). We provide an \RR{} \citep{R2007} package called 
\Rpackage{multcomp} \citep{pkg:multcomp} implementing
simultaneous tests and confidence intervals for a wide range of models which
is based on Alan Genz' implementations of the multivariate
normal and $t$ distributions available in \RR{} from package \Rpackage{mvtnorm} 
\citep{pkg:mvtnorm,Rnews:Hothorn+Bretz+Genz:2001}. 

In \RR, estimated model coefficients $\hat{\vartheta}$ and their covariance 
matrix $\hat{\Sigma}$ are accessible by the model's \Rcmd{coef()} 
and \Rcmd{vcov()} methods. Examples include objects of class \Rclass{lm},
\Rclass{glm}, \Rclass{coxph}, \Rclass{nlme}, \Rclass{survreg} and many more.
Thus, it is sufficient to assume both
methods being available for the model of interest. The \underline{g}eneral
\underline{l}inear \underline{h}ypo\underline{t}hesis for a model 
`\Robject{model}' and a representation of the matrix $\K$ (via its
\Robject{linfct} argument)
is set up using \Rcmd{glht()}:
\begin{Sinput}
glht(model, linfct, alternative = c("two.sided", "less", "greater"), 
     rhs = 0, ...)
\end{Sinput}
The two remaining arguments
\Rarg{alternative} and \Rarg{rhs} define the direction
of the alternative (see Section~\ref{siminf}) and $\m$, respectively.
The matrix $\K$ can be described in three ways
\begin{itemize}
\item by a matrix with \Rcmd{length(coef(model))} columns,
\item by an expression or character vector giving a symbolic description 
      of the linear functions of interest or
\item by an object of class \Rclass{mcp} 
      (for \underline{m}ultiple \underline{c}omparison \underline{p}rocedure).
\end{itemize}
The latter one is convenient when contrasts of levels of a factor are
to be compared and the model contrasts used to define the design matrix
of the model have to be taken into account. The \Rcmd{mcp()} function
takes the name of the factor under test as argument and a character 
defining the type of comparisons as its value. For example,
\Rcmd{mcp(type = "Tukey")} sets up a matrix $\K$ for Tukey's honest
significant differences in all levels of a factor \Robject{type}
which has to be part of right hand side of the model formula
of \Robject{model}. In this particular case, we need to assume
that \Rcmd{model.frame()} and \Rcmd{model.matrix()} methods for 
\Robject{model} are available as well. 

Objects of class \Rclass{glht} returned by \Rcmd{glht()} are equiped with
\Rcmd{coef()} and \Rcmd{vcov()} methods for computing
$\K \hat{\vartheta}$ and $\hat{\S}$. Furthermore, a \Rcmd{summary()} method
is available for performing various flavors of tests (maximum-type, $\chi^2$ and 
$F$-tests) and adjusted $p$-values including those taking 
logical constraints into account \citep{Shaffer1986, Westfall1997}.
In addition, the \Rcmd{confint()} method applied to objects of class 
\Rclass{glht} returns simultaneous confidence intervals and allows
for a graphical representation of the results. Adjusted $p$-values and
simultaneous confidence intervals implemented in \Rpackage{multcomp}
are continuously checked against results reported by
\cite{Westfall1999}.

The technical details of the implementation are not very interesting
at this point and we hope that the basic principles become much
clearer by means of the illustrations presented in the next section.
The interested reader is refered to the online documentation shipped
with package \Rpackage{multcomp}.

\input{illustrations}

\input{trees}

\section{Discussion}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
